# Detailed Approach for Time Prediction Estimation from Sundial Images

## 1. Data Collection & Labeling

**Image Capture:**  
Capture images of a vertical stick (sundial) at different times of the day using a camera. Ensure that the stick is perpendicular to the ground to clearly capture the shadow.

**Labeling:**  
Record the actual time when each image is taken. To simplify, embed the time into the filename (e.g., “2030.jpg” or “20_30.png”) so that the time (in hours) can be extracted directly without needing a separate CSV file.

---

## 2. Data Preprocessing

**Loading Images:**  
Use libraries such as OpenCV or TensorFlow’s image processing utilities to load images from the dataset folder.

**Resizing & Normalization:**  
Resize each image to a fixed size (e.g., 128×128 pixels) and normalize pixel values to the [0, 1] range.

**Label Extraction:**  
Create a function that uses regular expressions to extract the time from the image filenames. Convert the extracted time into a decimal hour (e.g., “2030” becomes 20.5).

---

## 3. Handling the Cyclic Nature of Time

**Cyclic Encoding:**  
Time-of-day is cyclic (e.g., 23:00 is very near to 01:00). To address this:

- **Convert Time into Sine and Cosine Components:**  
  For a given time `t` in hours, compute:
  
  - `t_sin = sin(2π * t / 24)`
  - `t_cos = cos(2π * t / 24)`
  
  This representation allows the model to learn the periodicity of time. The predicted cyclic components can later be decoded back to time.

---

## 4. Dataset Creation Using TensorFlow

**tf.data.Dataset Pipeline:**  
Create a TensorFlow dataset from the list of image file paths and their corresponding encoded labels. Use mapping functions to load and preprocess images.

**Dataset Splitting:**  
Split the dataset into training and validation sets (e.g., an 80/20 split).

**Data Augmentation:**  
Apply data augmentation techniques (e.g., random brightness and contrast adjustments) to the training set to improve generalization.

---

## 5. Model Building with Transfer Learning

**Leveraging Pre-trained Models:**  
Use a convolutional neural network (CNN) like VGG16 pre-trained on ImageNet. Freeze the base layers to prevent altering learned features during initial training.

**Custom Head for Regression:**  
Add additional layers (e.g., Global Average Pooling, Dense layers, Dropout) on top of the base model. The final output layer should have two neurons with a tanh activation to predict the sine and cosine components.

**Loss Function & Optimization:**  
Use Mean Squared Error (MSE) as the loss function since this is a regression problem. Employ an optimizer like Adam for training.

---

## 6. Training & Cross-Validation

**Training:**  
Train the model on the training dataset while validating performance on the validation set. Use callbacks such as EarlyStopping (to prevent overfitting) and ReduceLROnPlateau (to adjust the learning rate when improvements stall).

**Cross-Validation (Optional):**  
For a more robust estimate of performance, you might also perform K-Fold cross-validation if your dataset size permits.

---

## 7. Evaluation & Prediction

**Evaluation:**  
After training, evaluate the model on the validation set to measure the MSE and MAE.

**Decoding Predictions:**  
Convert the predicted sine and cosine values back into the time in hours using the inverse trigonometric function:

